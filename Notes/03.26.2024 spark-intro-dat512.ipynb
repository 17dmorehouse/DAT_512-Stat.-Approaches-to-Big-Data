{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spark Intro for DAT 513\n\nbased of off\n\nChapter 3 \"Learning Spark, 2nd Edition\",  Damji, Lee, Wenig, Das, from O'Reilly ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-14T19:48:09.281412Z","iopub.execute_input":"2022-03-14T19:48:09.281998Z","iopub.status.idle":"2022-03-14T19:48:09.295179Z","shell.execute_reply.started":"2022-03-14T19:48:09.281960Z","shell.execute_reply":"2022-03-14T19:48:09.294346Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#install Apache Spark\n!pip install pyspark --quiet","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:48:21.028282Z","iopub.execute_input":"2022-03-14T19:48:21.028596Z","iopub.status.idle":"2022-03-14T19:49:10.309348Z","shell.execute_reply.started":"2022-03-14T19:48:21.028565Z","shell.execute_reply":"2022-03-14T19:49:10.308214Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Apache Spark Libraries\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n#Apache Spark ML CLassifier Libraries\nfrom pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier,NaiveBayes\n\n#Apache Spark Evaluation Library\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n#Apache Spark Features libraries\nfrom pyspark.ml.feature import StandardScaler,StringIndexer, VectorAssembler, VectorIndexer, OneHotEncoder\n\n#Apache Spark Pipelin Library\nfrom pyspark.ml import Pipeline\n\n# Apache Spark `DenseVector`\nfrom pyspark.ml.linalg import DenseVector\n\n#Data Split Libraries\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n\n#Tabulating Data\nfrom tabulate import tabulate\n\n#Garbage\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:49:13.576084Z","iopub.execute_input":"2022-03-14T19:49:13.576495Z","iopub.status.idle":"2022-03-14T19:49:14.962860Z","shell.execute_reply.started":"2022-03-14T19:49:13.576447Z","shell.execute_reply":"2022-03-14T19:49:14.961914Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Building Spark Session\nspark = (SparkSession.builder\n                  .appName('Author_ages')\n                  .config(\"spark.executor.memory\", \"1G\")\n                  .config(\"spark.executor.cores\",\"4\")\n                  .getOrCreate())","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:49:22.506419Z","iopub.execute_input":"2022-03-14T19:49:22.506742Z","iopub.status.idle":"2022-03-14T19:49:28.884855Z","shell.execute_reply.started":"2022-03-14T19:49:22.506706Z","shell.execute_reply":"2022-03-14T19:49:28.883579Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Import the functions to handle the avg() calculation","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import *\nfrom pyspark.sql.types import *","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:49:37.562626Z","iopub.execute_input":"2022-03-14T19:49:37.563926Z","iopub.status.idle":"2022-03-14T19:49:37.569002Z","shell.execute_reply.started":"2022-03-14T19:49:37.563846Z","shell.execute_reply":"2022-03-14T19:49:37.567906Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame \ndata_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), \n  (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n# Group the same names together, aggregate their ages, and compute an average\navg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n# Show the results of the final execution\navg_df.show()\n#print(\"avg: \"+data_df.select(avg(\"age\")).collect(0)(0))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:49:40.850633Z","iopub.execute_input":"2022-03-14T19:49:40.851053Z","iopub.status.idle":"2022-03-14T19:49:47.507530Z","shell.execute_reply.started":"2022-03-14T19:49:40.851001Z","shell.execute_reply":"2022-03-14T19:49:47.506322Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Python works with schema,  much like databases do, to specify how data is stored\n\nWhen loading data or setting up a DataFrame,  we can allow Spark to do this, or specify the schema\n\nData types are much like Python data types with slight differences,   see \n\n\"Learning Spark, 2nd Edition\" by Damji, Lee, Wenig and Das,  from O'Reilly,  chapter 3\n","metadata":{}},{"cell_type":"code","source":"# In Python\nschema = \"author STRING, title STRING, pages INT\"","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:49:53.738438Z","iopub.execute_input":"2022-03-14T19:49:53.738742Z","iopub.status.idle":"2022-03-14T19:49:53.742936Z","shell.execute_reply.started":"2022-03-14T19:49:53.738713Z","shell.execute_reply":"2022-03-14T19:49:53.742033Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define schema for our data using DDL \nschema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n\n# Create our static data, as a list of lists in Python\ndata = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n\"LinkedIn\"]],\n       [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n\"LinkedIn\"]],\n       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n\"twitter\", \"FB\", \"LinkedIn\"]],\n       [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, \n[\"twitter\", \"FB\"]],\n       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n\"twitter\", \"FB\", \"LinkedIn\"]],\n       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, \n[\"twitter\", \"LinkedIn\"]]\n      ]\n\n# convert to a Spark DataFrame\n\n# Create a DataFrame using the schema defined above\nblogs_df = spark.createDataFrame(data, schema)\n\n# Show the DataFrame; it should reflect our table above\nblogs_df.show()\n# Print the schema used by Spark to process the DataFrame\nprint(blogs_df.printSchema())","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:49:59.094036Z","iopub.execute_input":"2022-03-14T19:49:59.094894Z","iopub.status.idle":"2022-03-14T19:49:59.569668Z","shell.execute_reply.started":"2022-03-14T19:49:59.094843Z","shell.execute_reply":"2022-03-14T19:49:59.568717Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"blogs_df.schema","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:10.989720Z","iopub.execute_input":"2022-03-14T19:50:10.990070Z","iopub.status.idle":"2022-03-14T19:50:10.999527Z","shell.execute_reply.started":"2022-03-14T19:50:10.990037Z","shell.execute_reply":"2022-03-14T19:50:10.998691Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"Many operations on Spark data frames appear to match operations in Python","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blogs_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:17.192709Z","iopub.execute_input":"2022-03-14T19:50:17.193322Z","iopub.status.idle":"2022-03-14T19:50:17.199755Z","shell.execute_reply.started":"2022-03-14T19:50:17.193283Z","shell.execute_reply":"2022-03-14T19:50:17.198797Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"The dir function seems to work the same way, showing us the variables and member functions","metadata":{}},{"cell_type":"code","source":"dir(blogs_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:20.086154Z","iopub.execute_input":"2022-03-14T19:50:20.087567Z","iopub.status.idle":"2022-03-14T19:50:20.096273Z","shell.execute_reply.started":"2022-03-14T19:50:20.087515Z","shell.execute_reply":"2022-03-14T19:50:20.095226Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"blogs_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:26.102928Z","iopub.execute_input":"2022-03-14T19:50:26.103430Z","iopub.status.idle":"2022-03-14T19:50:26.109502Z","shell.execute_reply.started":"2022-03-14T19:50:26.103389Z","shell.execute_reply":"2022-03-14T19:50:26.108718Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Row objects in Spark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import Row\nblog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", \n  [\"twitter\", \"LinkedIn\"])\n# access using index for individual items\nblog_row[1]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:31.458504Z","iopub.execute_input":"2022-03-14T19:50:31.459208Z","iopub.status.idle":"2022-03-14T19:50:31.466912Z","shell.execute_reply.started":"2022-03-14T19:50:31.459163Z","shell.execute_reply":"2022-03-14T19:50:31.465868Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"spark.version","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:34.786466Z","iopub.execute_input":"2022-03-14T19:50:34.786805Z","iopub.status.idle":"2022-03-14T19:50:34.794500Z","shell.execute_reply.started":"2022-03-14T19:50:34.786770Z","shell.execute_reply":"2022-03-14T19:50:34.793708Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Adding at data set, the Lyons-housing set again\n\nGo to Data on the Kaggle utilities (upper right),  go to \"add data\",   search for Lyons, and add the Lyons set","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T18:27:03.601279Z","iopub.execute_input":"2022-03-14T18:27:03.602828Z","iopub.status.idle":"2022-03-14T18:27:03.618600Z","shell.execute_reply.started":"2022-03-14T18:27:03.602751Z","shell.execute_reply":"2022-03-14T18:27:03.617705Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"In Spark\n\na \"Projection\" is a way to return only the rows matching a relational criterion,  the term \"Projection\" is from Relational Database terminology\n     we may call this a \"row selection, or a row slice\" in Python or R\n     \nProjections are done in Spark using the select() method - basically as you would in SQL\n\nthe conditions (or \"filter\") are specified in a filter() or where() method     --again, the where acts like SQL","metadata":{}},{"cell_type":"code","source":"url = '/kaggle/input/lyon-housing/lyon_housing.csv'\n\ndata = spark.read.format(\"csv\") \\\n       .option(\"header\", \"true\") \\\n       .option(\"inferSchema\",\"true\")\\\n       .load(url) \n\ndata.cache() #for faster re-use","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:50:42.881012Z","iopub.execute_input":"2022-03-14T19:50:42.881918Z","iopub.status.idle":"2022-03-14T19:50:45.026124Z","shell.execute_reply.started":"2022-03-14T19:50:42.881855Z","shell.execute_reply":"2022-03-14T19:50:45.025207Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:52:27.477382Z","iopub.execute_input":"2022-03-14T19:52:27.477778Z","iopub.status.idle":"2022-03-14T19:52:28.903811Z","shell.execute_reply.started":"2022-03-14T19:52:27.477741Z","shell.execute_reply":"2022-03-14T19:52:28.903196Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"maison_df=(data.select(\"surface_logement\",\"nombre_parkings\",\"prix\").where(col(\"type_bien\")==\"maison\"))\nmaison_df.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:55:24.687041Z","iopub.execute_input":"2022-03-14T19:55:24.687370Z","iopub.status.idle":"2022-03-14T19:55:24.996347Z","shell.execute_reply.started":"2022-03-14T19:55:24.687337Z","shell.execute_reply":"2022-03-14T19:55:24.995660Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"Calucluate the number of distinct neighborhoods","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"(data\n  .select(\"commune\")\n  .where(col(\"commune\").isNotNull())\n  .agg(countDistinct(\"commune\").alias(\"DistinctNeighborHoods\"))\n  .show())","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:57:23.157178Z","iopub.execute_input":"2022-03-14T19:57:23.157544Z","iopub.status.idle":"2022-03-14T19:57:24.035898Z","shell.execute_reply.started":"2022-03-14T19:57:23.157500Z","shell.execute_reply":"2022-03-14T19:57:24.034808Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"(data\n  .select(\"commune\")\n  .where(col(\"commune\").isNotNull())\n  .distinct()\n  .show())","metadata":{"execution":{"iopub.status.busy":"2022-03-14T19:58:59.638751Z","iopub.execute_input":"2022-03-14T19:58:59.639172Z","iopub.status.idle":"2022-03-14T19:59:00.061724Z","shell.execute_reply.started":"2022-03-14T19:58:59.639109Z","shell.execute_reply":"2022-03-14T19:59:00.060681Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"new_lyon=data.withColumnRenamed(\"commune\",\"Neibhorhood\")\n\nnew_lyon.show(5)\n\n(new_lyon.select('Neibhorhood','prix').where(col('prix')>700000).show(10))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T20:05:30.707937Z","iopub.execute_input":"2022-03-14T20:05:30.708277Z","iopub.status.idle":"2022-03-14T20:05:31.294408Z","shell.execute_reply.started":"2022-03-14T20:05:30.708241Z","shell.execute_reply":"2022-03-14T20:05:31.293476Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"lyon_date_df = (data\n  .withColumn(\"Trans_date\", to_timestamp(col(\"date_transaction\"), \"yyyy-mm-dd\"))\n  .drop(\"date_transaction\")) \n  \n                \n \nlyon_date_df.show(5)\n    \n#elect the converted columns\n(lyon_date_df.select(\"Trans_Date\",\"type_bien\",\"prix\").show(5, False))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T20:16:47.651791Z","iopub.execute_input":"2022-03-14T20:16:47.652158Z","iopub.status.idle":"2022-03-14T20:16:47.956479Z","shell.execute_reply.started":"2022-03-14T20:16:47.652108Z","shell.execute_reply":"2022-03-14T20:16:47.955366Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"lyon_date_df.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T20:15:12.585508Z","iopub.execute_input":"2022-03-14T20:15:12.585795Z","iopub.status.idle":"2022-03-14T20:15:12.733441Z","shell.execute_reply.started":"2022-03-14T20:15:12.585765Z","shell.execute_reply":"2022-03-14T20:15:12.732414Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"With the date set, we can order the data by year, month and price","metadata":{}},{"cell_type":"code","source":"(lyon_date_df\n  .select(year('Trans_date'),month('Trans_date'),\"prix\")\n  .distinct()\n  .orderBy(year('Trans_date'),month('Trans_date'),col(\"prix\"))\n  .show(10))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T20:20:44.697873Z","iopub.execute_input":"2022-03-14T20:20:44.699561Z","iopub.status.idle":"2022-03-14T20:20:45.506167Z","shell.execute_reply.started":"2022-03-14T20:20:44.699512Z","shell.execute_reply":"2022-03-14T20:20:45.505218Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"(lyon_date_df\n  .select(\"commune\")\n  .where(col(\"commune\").isNotNull())\n  .groupBy(\"commune\")\n  .count()\n  .orderBy(\"count\", ascending=False)\n  .show(n=10, truncate=False))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T20:25:18.528942Z","iopub.execute_input":"2022-03-14T20:25:18.529841Z","iopub.status.idle":"2022-03-14T20:25:19.326584Z","shell.execute_reply.started":"2022-03-14T20:25:18.529775Z","shell.execute_reply":"2022-03-14T20:25:19.325532Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import pyspark.sql.functions as F\n(lyon_date_df\n  .select(F.avg(\"prix\"), F.avg(\"surface_logement\"),\n    F.min(\"prix\"), F.max(\"prix\"))\n  .show())","metadata":{"execution":{"iopub.status.busy":"2022-03-14T20:27:24.609218Z","iopub.execute_input":"2022-03-14T20:27:24.609607Z","iopub.status.idle":"2022-03-14T20:27:25.002381Z","shell.execute_reply.started":"2022-03-14T20:27:24.609564Z","shell.execute_reply":"2022-03-14T20:27:25.001083Z"},"trusted":true},"execution_count":46,"outputs":[]}]}